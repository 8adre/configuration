---
#
# edX Configuration
#
# github:     https://github.com/edx/configuration
# wiki:       https://github.com/edx/configuration/wiki
# code style: https://github.com/edx/configuration/wiki/Ansible-Coding-Conventions
# license:    https://github.com/edx/configuration/blob/master/LICENSE.TXT
#
#
#
# Tasks for role analytics_pipeline
# 
# Overview:
# 
# Prepare the machine to run the edX Analytics Data Pipeline. The pipeline currently "installs itself"
# via an ansible playbook that is not included in the edx/configuration repo. However, in order to
# run the pipeline in a devstack environment, some configuration needs to be performed. In a production
# environment many of these config files are stored on S3.
#
# Dependencies:
#
# common: some of the variables from the common role are used here
# hadoop_master: ensures hadoop services are installed
# hive: the pipeline makes extensive usage of hive, so that needs to be installed as well
# sqoop: similarly to hive, the pipeline uses this tool extensively
# 
# Example play:
#
# - name: Deploy all dependencies of edx-analytics-pipeline to the node
#   hosts: all
#   sudo: True
#   gather_facts: True
#   roles:
#     - analytics_pipeline
#
# ansible-playbook -i 'localhost,' ./analytics_pipeline.yml  -e@/ansible/vars/deployment.yml -e@/ansible/vars/env-deployment.yml
#

- name: ensure system packages are installed
  command: >
    make system-requirements
    chdir={{ analytics_pipeline_code_dir }}
  sudo: True
  tags:
    - install
    - install:system-requirements

- name: build virtualenv
  command: "virtualenv --python={{ ANALYTICS_PIPELINE_PYTHON }} {{ analytics_pipeline_venv_dir }}"
  args:
    creates: "{{ analytics_pipeline_venv_dir }}/bin/pip"
  sudo_user: "{{ analytics_pipeline_user }}"
  tags:
    - install
    - install:system-requirements

- name: installed
  shell: >
    . {{ analytics_pipeline_venv_dir }}/bin/activate && make install
    chdir={{ analytics_pipeline_code_dir }}
  environment: analytics_pipeline_install_env
  tags:
    - install
    - install:app-requirements

- name: converted to development mode
  shell: >
    . {{ analytics_pipeline_venv_dir }}/bin/activate && make uninstall && make develop
    chdir={{ analytics_pipeline_code_dir }}
  environment: analytics_pipeline_install_env
  tags:
    - devstack
    - devstack:install

- name: development config installed
  file: >
    src={{ analytics_pipeline_code_dir }}/config/devstack.cfg
    dest={{ analytics_pipeline_code_dir }}/override.cfg
    owner={{ analytics_pipeline_user }} group={{ analytics_pipeline_user }}
    state=link
  tags:
    - devstack
    - devstack:install

- name: create log directory
  file: >
    path="{{ analytics_pipeline_log_dir }}"
    mode=0777 owner={{ analytics_pipeline_user }} group={{ analytics_pipeline_user }}
    state=directory
  tags:
    - install
    - install:configuration

- name: logging configured
  template: >
    src=logging.cfg.j2
    dest={{ analytics_pipeline_code_dir }}/logging.cfg
  tags:
    - install
    - install:configuration

- name: create config directory
  file: >
    path="{{ analytics_pipeline_config_dir }}"
    mode=0755 owner={{ analytics_pipeline_user }} group={{ analytics_pipeline_user }}
    state=directory
  tags:
    - install
    - install:configuration

- name: store output database credentials for analytics pipeline
  copy: >
    content="{{ ANALYTICS_PIPELINE_OUTPUT_DATABASE | to_json }}"
    dest={{ analytics_pipeline_config_dir }}/output.json
    mode=0644 owner={{ analytics_pipeline_user }} group={{ analytics_pipeline_user }}
  tags:
    - install
    - install:configuration

- name: store input database credentials for analytics pipeline
  copy: >
    content="{{ ANALYTICS_PIPELINE_INPUT_DATABASE | to_json }}"
    dest={{ analytics_pipeline_config_dir }}/input.json
    mode=0644 owner={{ analytics_pipeline_user }} group={{ analytics_pipeline_user }}
  tags:
    - install
    - install:configuration

- name: luigi configuration directory created
  file: >
    path=/etc/luigi
    state=directory
    mode=755
  tags:
    - install
    - install:configuration

- name: luigi configuration file written
  template: >
    src=client.cfg.j2
    dest=/etc/luigi/client.cfg
    mode=644
  tags:
    - install
    - install:configuration

- name: util library source checked out
  git: >
    dest={{ analytics_pipeline_util_library.path }} repo={{ analytics_pipeline_util_library.repo }}
    version={{ analytics_pipeline_util_library.version }}
  tags:
    - install
    - install:code

- name: lib directory created
  file: >
    path={{ HADOOP_COMMON_USER_HOME }}/lib
    owner={{ hadoop_common_user }} group={{ hadoop_common_group }} state=directory
  tags:
    - install
    - install:app-requirements

- name: check if the util library needs to be built
  stat: >
    path={{ HADOOP_COMMON_USER_HOME }}/lib/edx-analytics-hadoop-util.jar
  register: util_lib_built
  tags:
    - install
    - install:app-requirements

- name: util library built
  shell: >
    chdir={{ analytics_pipeline_util_library.path }}
    {{ hadoop_common_java_home }}/bin/javac -cp `{{ HADOOP_COMMON_HOME }}/bin/hadoop classpath` org/edx/hadoop/input/ManifestTextInputFormat.java &&
    {{ hadoop_common_java_home }}/bin/jar cf {{ HADOOP_COMMON_USER_HOME }}/lib/edx-analytics-hadoop-util.jar org/edx/hadoop/input/ManifestTextInputFormat.class &&
    chown {{ hadoop_common_user }}:{{ hadoop_common_group }} {{ HADOOP_COMMON_USER_HOME }}/lib/edx-analytics-hadoop-util.jar
  when: not util_lib_built.stat.exists
  tags:
    - install
    - install:app-requirements

- name: env vars sourced in hadoop env
  lineinfile: >
    dest={{ hadoop_common_env }}
    regexp="^. {{ analytics_pipeline_venv_dir }}/bin/activate"
    line=". {{ analytics_pipeline_venv_dir }}/bin/activate"
    state=present
  tags:
    - install
    - install:configuration

- name: write devstack script
  template:
    src: "edx/app/analytics_pipeline/devstack.sh.j2"
    dest: "{{ analytics_pipeline_home }}/devstack.sh"
    owner: "{{ analytics_pipeline_user }}"
    group: "{{ analytics_pipeline_user }}"
    mode: 0744
  tags:
    - devstack
    - devstack:install

- name: create data directory
  file: >
    path="{{ ANALYTICS_PIPELINE_LOCAL_DATA_DIR }}"
    mode=0777 owner={{ analytics_pipeline_user }} group={{ analytics_pipeline_user }}
    state=directory
  tags:
    - devstack
    - devstack:install

- name: ensure tracking log file can be read
  file: >
    path={{ ANALYTICS_PIPELINE_LOCAL_DATA_DIR }}/tracking.log
    mode=0644
  ignore_errors: yes
  tags:
    - devstack
    - devstack:install

- name: cron job syncs tracking log file to hdfs
  cron: >
    user={{ hadoop_common_user }}
    name="Sync tracking log to HDFS"
    job="{{ HADOOP_COMMON_HOME }}/bin/hdfs dfs -put -f {{ ANALYTICS_PIPELINE_LOCAL_DATA_DIR }}/tracking.log {{ ANALYTICS_PIPELINE_HDFS_DATA_DIR }}/tracking.log"
  tags:
    - devstack
    - devstack:install

- name: wait for database
  wait_for:
    host: "{{ ANALYTICS_PIPELINE_OUTPUT_DATABASE.host }}"
    port: "{{ ANALYTICS_PIPELINE_OUTPUT_DATABASE.port }}"
    delay: 2
  tags:
    - devstack
    - devstack:migrate

- name: create database users
  mysql_user:
    login_host: "{{ ANALYTICS_PIPELINE_OUTPUT_DATABASE.host }}"
    login_port: "{{ ANALYTICS_PIPELINE_OUTPUT_DATABASE.port }}"
    login_user: "{{ ANALYTICS_PIPELINE_DB_ADMIN_USER }}"
    login_password: "{{ ANALYTICS_PIPELINE_DB_ADMIN_PASSWORD }}"
    name: "{{ ANALYTICS_PIPELINE_OUTPUT_DATABASE.username }}"
    host: "%"
    password: "{{ ANALYTICS_PIPELINE_OUTPUT_DATABASE.password }}"
    priv: "{{ ANALYTICS_PIPELINE_OUTPUT_DATABASE_NAME }}.*:ALL"
  tags:
    - devstack
    - devstack:migrate
